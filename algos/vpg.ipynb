{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete\n",
    "import time\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "functions in core.py\n",
    "\"\"\"\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input:\n",
    "      vector x,\n",
    "      [x0,\n",
    "       x1,\n",
    "       x2]\n",
    "    output:\n",
    "      [x0 + discount * x1 + discount^2 * x2,\n",
    "       x1 + discount * x2,\n",
    "       x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "def gaussian_likelihood(x, mu, log_std):\n",
    "    pre_sum = -0.5 * (((x - mu) / (tf.exp(log_std) + EPS))**2 + 2 * log_std + np.log(2 * np.pi))\n",
    "    return tf.reduce_sum(pre_sum, axis=1)\n",
    "\n",
    "\n",
    "def make_mlp_model(input_shape, sizes, activation='tanh', output_activation=None):\n",
    "    \"\"\" Build a feedforward neural network. \"\"\"\n",
    "    mlp = tf.keras.Sequential()\n",
    "    mlp.add(tf.keras.layers.Dense(sizes[0], activation=activation, input_shape=(input_shape,)))\n",
    "    if len(sizes) > 2:\n",
    "        for size in sizes[1:-1]:\n",
    "            mlp.add(tf.keras.layers.Dense(size, activation=activation))\n",
    "\n",
    "    mlp.add(tf.keras.layers.Dense(sizes[-1], activation=output_activation))\n",
    "    return mlp\n",
    "\n",
    "def statistics_scalar(x):\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    return np.mean(x), np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Actor-Critics\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self, obs_dim, hidden_sizes=(64, 64), activation='tanh', output_activation=None, action_space=None, pi_lr=3e-4, vf_lr=1e-3, train_v_iters=80):\n",
    "        if isinstance(action_space, Box):\n",
    "            act_dim = len(action_space.sample())\n",
    "            self.pi_mlp = make_mlp_model(obs_dim, list(hidden_sizes) + [act_dim], activation, output_activation)\n",
    "            self.policy = self._mlp_gaussian_policy\n",
    "            self.log_std = tf.Variable(name='log_std', initial_value=-0.5 * np.ones(act_dim, dtype=np.float32))\n",
    "\n",
    "        elif isinstance(action_space, Discrete):\n",
    "            act_dim = action_space.n\n",
    "            self.pi_mlp = make_mlp_model(obs_dim, list(hidden_sizes) + [act_dim], activation, None)\n",
    "            self.policy = self._mlp_categorical_policy\n",
    "            self.action_space = action_space\n",
    "        self.v_mlp = make_mlp_model(obs_dim, list(hidden_sizes) + [1], activation, None)\n",
    "        # optimizers\n",
    "        self.pi_optimizer = tf.optimizers.Adam(learning_rate=pi_lr)\n",
    "        self.vf_optimizer = tf.optimizers.Adam(learning_rate=vf_lr)\n",
    "        self.train_v_iters = train_v_iters\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, observation, action):\n",
    "        pi, logp_pi = self.policy(observation, action)\n",
    "        v = tf.squeeze(self.v_mlp(observation), axis=1)\n",
    "        return pi, logp_pi, v\n",
    "\n",
    "    def _mlp_categorical_policy(self, observation, action):\n",
    "        act_dim = self.action_space.n\n",
    "        logits = self.pi_mlp(observation)\n",
    "        logp_all = tf.nn.log_softmax(logits)\n",
    "        pi = tfd.Categorical(logits).sample()  # pi is the next action\n",
    "        if action is not None:\n",
    "            action = tf.cast(action, tf.int32)\n",
    "            logp = tf.reduce_sum(tf.one_hot(action, act_dim) * logp_all, axis=1)\n",
    "        else:\n",
    "            logp = tf.reduce_sum(tf.one_hot(pi, act_dim) * logp_all, axis=1)\n",
    "        return pi, logp\n",
    "\n",
    "    def _mlp_gaussian_policy(self, observation, action):\n",
    "        mu = self.pi_mlp(observation)\n",
    "        std = tf.exp(self.log_std)\n",
    "        pi = mu + tf.random.normal(tf.shape(mu)) * std  # pi is the next action\n",
    "        if action is not None:\n",
    "            logp = gaussian_likelihood(action, mu, self.log_std)\n",
    "        else:\n",
    "            logp = gaussian_likelihood(pi, mu, self.log_std)\n",
    "        return pi, logp\n",
    "\n",
    "    def update(self, buf):\n",
    "        obs_buf, act_buf, adv_buf, ret_buf, logp_buf = buf.get()\n",
    "\n",
    "        with tf.GradientTape() as pi_tape, tf.GradientTape() as vf_tape:\n",
    "            pi, logp, v = self.__call__(obs_buf, act_buf)\n",
    "\n",
    "            pi_loss = -tf.reduce_mean(logp * adv_buf)\n",
    "            v_loss = tf.reduce_mean((ret_buf - v) ** 2)\n",
    "\n",
    "        if hasattr(self, 'log_std'):\n",
    "            all_trainable_variables = [self.log_std, *self.pi_mlp.trainable_variables]\n",
    "            pi_grads = pi_tape.gradient(pi_loss, all_trainable_variables)\n",
    "            self.pi_optimizer.apply_gradients(zip(pi_grads, all_trainable_variables))\n",
    "        else:\n",
    "            pi_grads = pi_tape.gradient(pi_loss, self.pi_mlp.trainable_variables)\n",
    "            self.pi_optimizer.apply_gradients(zip(pi_grads, self.pi_mlp.trainable_variables))\n",
    "\n",
    "        vf_grads = vf_tape.gradient(v_loss, self.v_mlp.trainable_variables)\n",
    "        for _ in range(self.train_v_iters):\n",
    "            self.vf_optimizer.apply_gradients(zip(vf_grads, self.v_mlp.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a VPG agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        return self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, self.logp_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vanilla Policy Gradient\n",
    "(with GAE-Lambda for advantage estimation)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def vpg(env, ac_kwargs=None, seed=0, steps_per_epoch=4000, epochs=50, gamma=0.99, lam=0.97, max_ep_len=1000, save_freq=10):\n",
    "\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # Create actor-critic agent \n",
    "    ac_kwargs['action_space'] = env.action_space\n",
    "    ac_kwargs['obs_dim'] = env.observation_space.shape[0]\n",
    "\n",
    "    actor_critic = ActorCritic(**ac_kwargs)\n",
    "\n",
    "\n",
    "    # Experience buffer\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "    buf = VPGBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
    "\n",
    "    \"\"\"\n",
    "    Main loop: collect experience in env and update/log each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # o for observation, r for reward, d for done\n",
    "    o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "    all_ep_ret = []\n",
    "    summary_ep_ret = []\n",
    "    totalEnvInteracts = []\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(steps_per_epoch):\n",
    "            a, logp_t, v_t = actor_critic(o.reshape(1, -1), None)\n",
    "\n",
    "            # save and log\n",
    "            a = a.numpy()[0]\n",
    "            buf.store(o, a, r, v_t, logp_t)\n",
    "\n",
    "            o, r, d, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            terminal = d or (ep_len == max_ep_len)\n",
    "            if terminal or (t == steps_per_epoch - 1):\n",
    "                if not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.' % ep_len)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                last_val = r if d else v_t\n",
    "                buf.finish_path(last_val)\n",
    "\n",
    "                if terminal:\n",
    "                    all_ep_ret.append(ep_ret)\n",
    "                # reset environment\n",
    "                o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "        # Perform VPG update!\n",
    "        actor_critic.update(buf)\n",
    "        mean, std = statistics_scalar(all_ep_ret)\n",
    "        all_ep_ret = []\n",
    "\n",
    "        print(f'epoch {epoch}: mean {mean}, std {std}')\n",
    "        summary_ep_ret.append(mean)\n",
    "        totalEnvInteracts.append((epoch + 1) * steps_per_epoch)\n",
    "\n",
    "\n",
    "    plt.plot(totalEnvInteracts, summary_ep_ret)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
=======
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vpg.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
>>>>>>> c47452224ac545f730496da702b317e5e105ea01
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelguan1992/spinningup-in-deeprl-tensorflow2/blob/master/algos/vpg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2kbrJGOD8aha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow==2.0.0-alpha0\n",
        "!pip install -q tfp-nightly\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import gym\n",
        "from gym.spaces import Box, Discrete\n",
        "import time\n",
        "\n",
        "import scipy.signal\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OYL5HDm_8s_Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "functions in core.py\n",
        "\"\"\"\n",
        "EPS = 1e-8\n",
        "\n",
        "\n",
        "def combined_shape(length, shape=None):\n",
        "  if shape is None:\n",
        "    return (length,)\n",
        "  return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
        "\n",
        "\n",
        "def discount_cumsum(x, discount):\n",
        "  \"\"\"\n",
        "  magic from rllab for computing discounted cumulative sums of vectors.\n",
        "  input:\n",
        "    vector x,\n",
        "    [x0,\n",
        "     x1,\n",
        "     x2]\n",
        "  output:\n",
        "    [x0 + discount * x1 + discount^2 * x2,\n",
        "     x1 + discount * x2,\n",
        "     x2]\n",
        "  \"\"\"\n",
        "  return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "def gaussian_likelihood(x, mu, log_std):\n",
        "  pre_sum = -0.5 * (((x - mu) / (tf.exp(log_std) + EPS))**2 + 2 * log_std + np.log(2 * np.pi))\n",
        "  return tf.reduce_sum(pre_sum, axis=1)\n",
        "  \n",
        "class MLP(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               sizes, \n",
        "               activation='tanh', \n",
        "               output_activation=None,\n",
        "               is_continue_action=False,\n",
        "               act_dim=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.denses = [tf.keras.layers.Dense(size, activation=activation) for size in sizes[:-1]]\n",
        "    self.out = tf.keras.layers.Dense(sizes[-1], activation=output_activation)\n",
        "    if is_continue_action:\n",
        "      if act_dim is None:\n",
        "        raise TypeError(\"__init__() missing 1 argument: 'act_dim' when log_std=True\")\n",
        "      self.log_std = tf.Variable(name='log_std', initial_value=-0.5 * np.ones(act_dim, dtype=np.float32))\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    for dense in self.denses:\n",
        "      x = dense(x)      \n",
        "    return self.out(x)\n",
        "\n",
        "def statistics_scalar(x):\n",
        "  x = np.array(x, dtype=np.float32)\n",
        "  return np.mean(x), np.std(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qdpZP_HdCdx7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "dc9e6277-b294-42e1-cadf-656cfd909401"
      },
      "cell_type": "code",
      "source": [
        "mlp = MLP([10,3], is_continue_action=True, act_dim=10)\n",
        "inputs = np.ones((10, 1))\n",
        "out = mlp(inputs)\n",
        "print(mlp.log_std)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'log_std:0' shape=(10,) dtype=float32, numpy=\n",
            "array([-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5],\n",
            "      dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "59XaWHsg8u2N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Actor-Critics\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ActorCritic:\n",
        "  def __init__(self, hidden_sizes=(64, 64), activation='tanh', output_activation=None, action_space=None, pi_lr=3e-4, vf_lr=1e-3, train_v_iters=80):\n",
        "    if isinstance(action_space, Box):\n",
        "      act_dim = len(action_space.sample())\n",
        "      self.pi_mlp = MLP(list(hidden_sizes) + [act_dim], activation, output_activation, is_continue_action=True)\n",
        "      self.policy = self._mlp_gaussian_policy\n",
        "\n",
        "    elif isinstance(action_space, Discrete):\n",
        "      act_dim = action_space.n\n",
        "      self.pi_mlp = MLP(list(hidden_sizes) + [act_dim], activation, None)\n",
        "      self.policy = self._mlp_categorical_policy      \n",
        "      self.action_space = action_space\n",
        "    self.v_mlp = MLP(list(hidden_sizes) + [1], activation, None)\n",
        "    \n",
        "    # optimizers\n",
        "    self.pi_optimizer = tf.optimizers.Adam(learning_rate=pi_lr)\n",
        "    self.vf_optimizer = tf.optimizers.Adam(learning_rate=vf_lr)\n",
        "    self.train_v_iters = train_v_iters\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, observation, action):\n",
        "    pi, logp_pi = self.policy(observation, action)\n",
        "    v = tf.squeeze(self.v_mlp(observation), axis=1)\n",
        "    return pi, logp_pi, v\n",
        "\n",
        "  def _mlp_categorical_policy(self, observation, action):\n",
        "    act_dim = self.action_space.n\n",
        "    logits = self.pi_mlp(observation)\n",
        "    logp_all = tf.nn.log_softmax(logits)\n",
        "    pi = tfd.Categorical(logits).sample()  # pi is the next action\n",
        "    if action is not None:\n",
        "      action = tf.cast(action, tf.int32)\n",
        "      logp = tf.reduce_sum(tf.one_hot(action, act_dim, dtype=tf.float64) * logp_all, axis=1)\n",
        "    else:\n",
        "      logp = tf.reduce_sum(tf.one_hot(pi, act_dim, dtype=tf.float64) * logp_all, axis=1)\n",
        "    return pi, logp\n",
        "\n",
        "  def _mlp_gaussian_policy(self, observation, action):\n",
        "    mu = self.pi_mlp(observation)\n",
        "    std = tf.exp(self.pi_mlp.log_std)\n",
        "    pi = mu + tf.random.normal(tf.shape(mu)) * std  # pi is the next action\n",
        "    if action is not None:\n",
        "      logp = gaussian_likelihood(action, mu, self.pi_mlp.log_std)\n",
        "    else:\n",
        "      logp = gaussian_likelihood(pi, mu, self.pi_mlp.log_std)\n",
        "    return pi, logp\n",
        "\n",
        "  def update(self, buf):\n",
        "    obs_buf, act_buf, adv_buf, ret_buf, logp_buf = buf.get()\n",
        "\n",
        "    with tf.GradientTape() as pi_tape, tf.GradientTape() as vf_tape:\n",
        "      pi, logp, v = self.__call__(obs_buf, act_buf)\n",
        "\n",
        "      pi_loss = -tf.reduce_mean(logp * adv_buf)\n",
        "      v_loss = tf.reduce_mean((ret_buf - v) ** 2)\n",
        "\n",
        "    pi_grads = pi_tape.gradient(pi_loss, self.pi_mlp.trainable_variables)\n",
        "    self.pi_optimizer.apply_gradients(zip(pi_grads, self.pi_mlp.trainable_variables))\n",
        "\n",
        "    vf_grads = vf_tape.gradient(v_loss, self.v_mlp.trainable_variables)\n",
        "    for _ in range(self.train_v_iters):\n",
        "      self.vf_optimizer.apply_gradients(zip(vf_grads, self.v_mlp.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UK_GSAyG8zP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VPGBuffer:\n",
        "  \"\"\"\n",
        "  A buffer for storing trajectories experienced by a VPG agent interacting\n",
        "  with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
        "  for calculating the advantages of state-action pairs.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
        "    self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
        "    self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
        "    self.adv_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.rew_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.ret_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.val_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.logp_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.gamma, self.lam = gamma, lam\n",
        "    self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
        "\n",
        "  def store(self, obs, act, rew, val, logp):\n",
        "    \"\"\"\n",
        "    Append one timestep of agent-environment interaction to the buffer.\n",
        "    \"\"\"\n",
        "    assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
        "    self.obs_buf[self.ptr] = obs\n",
        "    self.act_buf[self.ptr] = act\n",
        "    self.rew_buf[self.ptr] = rew\n",
        "    self.val_buf[self.ptr] = val\n",
        "    self.logp_buf[self.ptr] = logp\n",
        "    self.ptr += 1\n",
        "\n",
        "  def finish_path(self, last_val=0):\n",
        "    \"\"\"\n",
        "    Call this at the end of a trajectory, or when one gets cut off\n",
        "    by an epoch ending. This looks back in the buffer to where the\n",
        "    trajectory started, and uses rewards and value estimates from\n",
        "    the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
        "    as well as compute the rewards-to-go for each state, to use as\n",
        "    the targets for the value function.\n",
        "    The \"last_val\" argument should be 0 if the trajectory ended\n",
        "    because the agent reached a terminal state (died), and otherwise\n",
        "    should be V(s_T), the value function estimated for the last state.\n",
        "    This allows us to bootstrap the reward-to-go calculation to account\n",
        "    for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
        "    \"\"\"\n",
        "\n",
        "    path_slice = slice(self.path_start_idx, self.ptr)\n",
        "    rews = np.append(self.rew_buf[path_slice], last_val)\n",
        "    vals = np.append(self.val_buf[path_slice], last_val)\n",
        "\n",
        "    # the next two lines implement GAE-Lambda advantage calculation\n",
        "    deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
        "    self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
        "\n",
        "    # the next line computes rewards-to-go, to be targets for the value function\n",
        "    self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
        "\n",
        "    self.path_start_idx = self.ptr\n",
        "\n",
        "  def get(self):\n",
        "    \"\"\"\n",
        "    Call this at the end of an epoch to get all of the data from\n",
        "    the buffer, with advantages appropriately normalized (shifted to have\n",
        "    mean zero and std one). Also, resets some pointers in the buffer.\n",
        "    \"\"\"\n",
        "    assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
        "    self.ptr, self.path_start_idx = 0, 0\n",
        "    # the next two lines implement the advantage normalization trick\n",
        "    adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
        "    self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
        "    return self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, self.logp_buf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cez9cWKy8zzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Vanilla Policy Gradient\n",
        "(with GAE-Lambda for advantage estimation)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def vpg(env, ac_kwargs=None, seed=0, steps_per_epoch=4000, epochs=50, gamma=0.99, lam=0.97, max_ep_len=1000, save_freq=10):\n",
        "\n",
        "  tf.random.set_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  # Create actor-critic agent \n",
        "  ac_kwargs['action_space'] = env.action_space\n",
        "#   ac_kwargs['obs_dim'] = env.observation_space.shape[0]\n",
        "\n",
        "  actor_critic = ActorCritic(**ac_kwargs)\n",
        "\n",
        "\n",
        "  # Experience buffer\n",
        "  obs_dim = env.observation_space.shape\n",
        "  act_dim = env.action_space.shape\n",
        "  buf = VPGBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
        "\n",
        "  \"\"\"\n",
        "  Main loop: collect experience in env and update/log each epoch\n",
        "  \"\"\"\n",
        "\n",
        "  # o for observation, r for reward, d for done\n",
        "  o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "\n",
        "  all_ep_ret = []\n",
        "  summary_ep_ret = []\n",
        "  totalEnvInteracts = []\n",
        "  for epoch in range(epochs):\n",
        "    for t in range(steps_per_epoch):\n",
        "      a, logp_t, v_t = actor_critic(o.reshape(1, -1), None)\n",
        "\n",
        "      # save and log\n",
        "      a = a.numpy()[0]\n",
        "      buf.store(o, a, r, v_t, logp_t)\n",
        "\n",
        "      o, r, d, _ = env.step(a)\n",
        "      ep_ret += r\n",
        "      ep_len += 1\n",
        "\n",
        "      terminal = d or (ep_len == max_ep_len)\n",
        "      if terminal or (t == steps_per_epoch - 1):\n",
        "        if not(terminal):\n",
        "          print('Warning: trajectory cut off by epoch at %d steps.' % ep_len)\n",
        "        # if trajectory didn't reach terminal state, bootstrap value target\n",
        "        last_val = r if d else v_t\n",
        "        buf.finish_path(last_val)\n",
        "\n",
        "        if terminal:\n",
        "          all_ep_ret.append(ep_ret)\n",
        "        # reset environment\n",
        "        o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "\n",
        "    # Perform VPG update!\n",
        "    actor_critic.update(buf)\n",
        "    mean, std = statistics_scalar(all_ep_ret)\n",
        "    all_ep_ret = []\n",
        "\n",
        "    print(f'epoch {epoch}: mean {mean}, std {std}')\n",
        "    summary_ep_ret.append(mean)\n",
        "    totalEnvInteracts.append((epoch + 1) * steps_per_epoch)\n",
        "\n",
        "  plt.plot(totalEnvInteracts, summary_ep_ret)\n",
        "  plt.grid(True)\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xVwDQk7b81n4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2728
        },
        "outputId": "ddbc9904-c63a-4747-87e8-75ce11fcb084"
      },
      "cell_type": "code",
      "source": [
        "vpg(gym.make('CartPole-v0'), ac_kwargs=dict(hidden_sizes=[64] * 2), gamma=0.99, seed=0, steps_per_epoch=4000, epochs=70)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: trajectory cut off by epoch at 12 steps.\n",
            "epoch 0: mean 20.989473342895508, std 11.072958946228027\n",
            "Warning: trajectory cut off by epoch at 6 steps.\n",
            "epoch 1: mean 22.69318199157715, std 10.855305671691895\n",
            "Warning: trajectory cut off by epoch at 6 steps.\n",
            "epoch 2: mean 22.564971923828125, std 10.68752384185791\n",
            "Warning: trajectory cut off by epoch at 27 steps.\n",
            "epoch 3: mean 21.592391967773438, std 10.073758125305176\n",
            "Warning: trajectory cut off by epoch at 23 steps.\n",
            "epoch 4: mean 21.851648330688477, std 10.636603355407715\n",
            "Warning: trajectory cut off by epoch at 34 steps.\n",
            "epoch 5: mean 24.633541107177734, std 12.606164932250977\n",
            "Warning: trajectory cut off by epoch at 6 steps.\n",
            "epoch 6: mean 25.602563858032227, std 13.408129692077637\n",
            "Warning: trajectory cut off by epoch at 9 steps.\n",
            "epoch 7: mean 24.63580322265625, std 11.070344924926758\n",
            "Warning: trajectory cut off by epoch at 19 steps.\n",
            "epoch 8: mean 26.1907901763916, std 13.473528861999512\n",
            "Warning: trajectory cut off by epoch at 15 steps.\n",
            "epoch 9: mean 30.189393997192383, std 16.796974182128906\n",
            "Warning: trajectory cut off by epoch at 18 steps.\n",
            "epoch 10: mean 27.65277862548828, std 14.385235786437988\n",
            "Warning: trajectory cut off by epoch at 89 steps.\n",
            "epoch 11: mean 28.97037124633789, std 14.647432327270508\n",
            "Warning: trajectory cut off by epoch at 8 steps.\n",
            "epoch 12: mean 28.719425201416016, std 15.199407577514648\n",
            "Warning: trajectory cut off by epoch at 5 steps.\n",
            "epoch 13: mean 29.813432693481445, std 14.907259941101074\n",
            "Warning: trajectory cut off by epoch at 3 steps.\n",
            "epoch 14: mean 31.472440719604492, std 15.06459903717041\n",
            "Warning: trajectory cut off by epoch at 23 steps.\n",
            "epoch 15: mean 33.141666412353516, std 17.916610717773438\n",
            "Warning: trajectory cut off by epoch at 7 steps.\n",
            "epoch 16: mean 30.71538543701172, std 14.716937065124512\n",
            "Warning: trajectory cut off by epoch at 9 steps.\n",
            "epoch 17: mean 35.00877380371094, std 17.88878631591797\n",
            "Warning: trajectory cut off by epoch at 41 steps.\n",
            "epoch 18: mean 34.42608642578125, std 18.735719680786133\n",
            "Warning: trajectory cut off by epoch at 8 steps.\n",
            "epoch 19: mean 32.19355010986328, std 17.372892379760742\n",
            "Warning: trajectory cut off by epoch at 10 steps.\n",
            "epoch 20: mean 35.625, std 17.7734375\n",
            "Warning: trajectory cut off by epoch at 19 steps.\n",
            "epoch 21: mean 36.52293395996094, std 15.402606964111328\n",
            "Warning: trajectory cut off by epoch at 16 steps.\n",
            "epoch 22: mean 38.67961120605469, std 22.317747116088867\n",
            "Warning: trajectory cut off by epoch at 27 steps.\n",
            "epoch 23: mean 35.47321319580078, std 19.82150650024414\n",
            "Warning: trajectory cut off by epoch at 21 steps.\n",
            "epoch 24: mean 39.790000915527344, std 18.7532901763916\n",
            "Warning: trajectory cut off by epoch at 23 steps.\n",
            "epoch 25: mean 38.61165237426758, std 16.985565185546875\n",
            "Warning: trajectory cut off by epoch at 54 steps.\n",
            "epoch 26: mean 37.94230651855469, std 20.874086380004883\n",
            "Warning: trajectory cut off by epoch at 46 steps.\n",
            "epoch 27: mean 39.540000915527344, std 17.05076026916504\n",
            "Warning: trajectory cut off by epoch at 64 steps.\n",
            "epoch 28: mean 39.75757598876953, std 16.77545738220215\n",
            "Warning: trajectory cut off by epoch at 23 steps.\n",
            "epoch 29: mean 41.8631591796875, std 22.90741539001465\n",
            "Warning: trajectory cut off by epoch at 34 steps.\n",
            "epoch 30: mean 40.8865966796875, std 19.321413040161133\n",
            "Warning: trajectory cut off by epoch at 20 steps.\n",
            "epoch 31: mean 40.20201873779297, std 21.659896850585938\n",
            "Warning: trajectory cut off by epoch at 15 steps.\n",
            "epoch 32: mean 46.882354736328125, std 21.27158546447754\n",
            "Warning: trajectory cut off by epoch at 1 steps.\n",
            "epoch 33: mean 40.806121826171875, std 21.651535034179688\n",
            "Warning: trajectory cut off by epoch at 58 steps.\n",
            "epoch 34: mean 45.83720779418945, std 20.485877990722656\n",
            "Warning: trajectory cut off by epoch at 67 steps.\n",
            "epoch 35: mean 46.82143020629883, std 22.522031784057617\n",
            "Warning: trajectory cut off by epoch at 99 steps.\n",
            "epoch 36: mean 50.66233825683594, std 23.836896896362305\n",
            "Warning: trajectory cut off by epoch at 1 steps.\n",
            "epoch 37: mean 47.04705810546875, std 21.064556121826172\n",
            "Warning: trajectory cut off by epoch at 29 steps.\n",
            "epoch 38: mean 46.17441940307617, std 17.76604652404785\n",
            "Warning: trajectory cut off by epoch at 2 steps.\n",
            "epoch 39: mean 47.595237731933594, std 20.949552536010742\n",
            "Warning: trajectory cut off by epoch at 32 steps.\n",
            "epoch 40: mean 53.621620178222656, std 30.616947174072266\n",
            "Warning: trajectory cut off by epoch at 10 steps.\n",
            "epoch 41: mean 49.875, std 22.48520278930664\n",
            "epoch 42: mean 51.28205108642578, std 24.397823333740234\n",
            "Warning: trajectory cut off by epoch at 46 steps.\n",
            "epoch 43: mean 49.42499923706055, std 22.994441986083984\n",
            "Warning: trajectory cut off by epoch at 21 steps.\n",
            "epoch 44: mean 49.12345504760742, std 22.636354446411133\n",
            "Warning: trajectory cut off by epoch at 37 steps.\n",
            "epoch 45: mean 49.537498474121094, std 24.776473999023438\n",
            "Warning: trajectory cut off by epoch at 47 steps.\n",
            "epoch 46: mean 48.20731735229492, std 21.14453125\n",
            "Warning: trajectory cut off by epoch at 67 steps.\n",
            "epoch 47: mean 57.0, std 23.841262817382812\n",
            "Warning: trajectory cut off by epoch at 28 steps.\n",
            "epoch 48: mean 56.74285888671875, std 25.08652687072754\n",
            "Warning: trajectory cut off by epoch at 6 steps.\n",
            "epoch 49: mean 51.8701286315918, std 22.728979110717773\n",
            "Warning: trajectory cut off by epoch at 16 steps.\n",
            "epoch 50: mean 57.739131927490234, std 26.032115936279297\n",
            "Warning: trajectory cut off by epoch at 9 steps.\n",
            "epoch 51: mean 56.21126937866211, std 25.86743927001953\n",
            "Warning: trajectory cut off by epoch at 82 steps.\n",
            "epoch 52: mean 59.3636360168457, std 32.7615966796875\n",
            "Warning: trajectory cut off by epoch at 34 steps.\n",
            "epoch 53: mean 58.32352828979492, std 35.63105010986328\n",
            "Warning: trajectory cut off by epoch at 13 steps.\n",
            "epoch 54: mean 61.338462829589844, std 29.085166931152344\n",
            "Warning: trajectory cut off by epoch at 52 steps.\n",
            "epoch 55: mean 61.6875, std 34.12013626098633\n",
            "Warning: trajectory cut off by epoch at 37 steps.\n",
            "epoch 56: mean 61.921875, std 29.83354949951172\n",
            "Warning: trajectory cut off by epoch at 36 steps.\n",
            "epoch 57: mean 60.984615325927734, std 32.28073501586914\n",
            "Warning: trajectory cut off by epoch at 2 steps.\n",
            "epoch 58: mean 63.4603157043457, std 32.829444885253906\n",
            "Warning: trajectory cut off by epoch at 54 steps.\n",
            "epoch 59: mean 63.64516067504883, std 32.36490249633789\n",
            "Warning: trajectory cut off by epoch at 32 steps.\n",
            "epoch 60: mean 62.0, std 32.13983154296875\n",
            "Warning: trajectory cut off by epoch at 20 steps.\n",
            "epoch 61: mean 71.07142639160156, std 31.870752334594727\n",
            "Warning: trajectory cut off by epoch at 122 steps.\n",
            "epoch 62: mean 68.03508758544922, std 36.03700256347656\n",
            "Warning: trajectory cut off by epoch at 6 steps.\n",
            "epoch 63: mean 68.86206817626953, std 34.91044998168945\n",
            "Warning: trajectory cut off by epoch at 16 steps.\n",
            "epoch 64: mean 72.43636322021484, std 38.97726821899414\n",
            "Warning: trajectory cut off by epoch at 86 steps.\n",
            "epoch 65: mean 88.95454406738281, std 41.02601623535156\n",
            "Warning: trajectory cut off by epoch at 30 steps.\n",
            "epoch 66: mean 90.2272720336914, std 44.750247955322266\n",
            "Warning: trajectory cut off by epoch at 38 steps.\n",
            "epoch 67: mean 88.04444122314453, std 46.21205139160156\n",
            "Warning: trajectory cut off by epoch at 15 steps.\n",
            "epoch 68: mean 110.69444274902344, std 57.471641540527344\n",
            "Warning: trajectory cut off by epoch at 148 steps.\n",
            "epoch 69: mean 93.95121765136719, std 54.625083923339844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOQW9iIif+/9PzSBo8O91QQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}