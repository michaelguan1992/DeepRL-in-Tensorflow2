{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vpg.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelguan1992/spinningup-in-deeprl-tensorflow2/blob/master/algos/vpg_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2kbrJGOD8aha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow==2.0.0-alpha0\n",
        "!pip install -q tfp-nightly\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import gym\n",
        "from gym.spaces import Box, Discrete\n",
        "import time\n",
        "\n",
        "import scipy.signal\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OYL5HDm_8s_Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "functions in core.py\n",
        "\"\"\"\n",
        "EPS = 1e-8\n",
        "\n",
        "\n",
        "def combined_shape(length, shape=None):\n",
        "  if shape is None:\n",
        "    return (length,)\n",
        "  return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
        "\n",
        "\n",
        "def discount_cumsum(x, discount):\n",
        "  \"\"\"\n",
        "  magic from rllab for computing discounted cumulative sums of vectors.\n",
        "  input:\n",
        "    vector x,\n",
        "    [x0,\n",
        "     x1,\n",
        "     x2]\n",
        "  output:\n",
        "    [x0 + discount * x1 + discount^2 * x2,\n",
        "     x1 + discount * x2,\n",
        "     x2]\n",
        "  \"\"\"\n",
        "  return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "def gaussian_likelihood(x, mu, log_std):\n",
        "  pre_sum = -0.5 * (((x - mu) / (tf.exp(log_std) + EPS))**2 + 2 * log_std + np.log(2 * np.pi))\n",
        "  return tf.reduce_sum(pre_sum, axis=1)\n",
        "  \n",
        "class MLP(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               sizes, \n",
        "               activation='tanh', \n",
        "               output_activation=None,\n",
        "               is_continue_action=False,\n",
        "               act_dim=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.denses = [tf.keras.layers.Dense(size, activation=activation) for size in sizes[:-1]]\n",
        "    self.out = tf.keras.layers.Dense(sizes[-1], activation=output_activation)\n",
        "    if is_continue_action:\n",
        "      if act_dim is None:\n",
        "        raise TypeError(\"__init__() missing 1 argument: 'act_dim' when log_std=True\")\n",
        "      self.log_std = tf.Variable(name='log_std', initial_value=-0.5 * np.ones(act_dim, dtype=np.float64))\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    for dense in self.denses:\n",
        "      x = dense(x)      \n",
        "    return self.out(x)\n",
        "\n",
        "def statistics_scalar(x):\n",
        "  x = np.array(x, dtype=np.float32)\n",
        "  return np.mean(x), np.std(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "59XaWHsg8u2N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Actor-Critics\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ActorCritic:\n",
        "  def __init__(self, hidden_sizes=(64, 64), activation='tanh', output_activation=None, action_space=None, pi_lr=3e-4, vf_lr=1e-3, train_v_iters=80):\n",
        "    if isinstance(action_space, Box):\n",
        "      act_dim = len(action_space.sample())\n",
        "      self.pi_mlp = MLP(list(hidden_sizes) + [act_dim], activation, output_activation, is_continue_action=True, act_dim=act_dim)\n",
        "      self.policy = self._mlp_gaussian_policy\n",
        "\n",
        "    elif isinstance(action_space, Discrete):\n",
        "      act_dim = action_space.n\n",
        "      self.pi_mlp = MLP(list(hidden_sizes) + [act_dim], activation, None)\n",
        "      self.policy = self._mlp_categorical_policy      \n",
        "      self.action_space = action_space\n",
        "\n",
        "    self.v_mlp = MLP(list(hidden_sizes) + [1], activation, None)\n",
        "    \n",
        "    # optimizers\n",
        "    self.pi_optimizer = tf.optimizers.Adam(learning_rate=pi_lr)\n",
        "    self.vf_optimizer = tf.optimizers.Adam(learning_rate=vf_lr)\n",
        "    self.train_v_iters = train_v_iters\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, observation, action):\n",
        "    pi, logp_pi = self.policy(observation, action)\n",
        "    v = tf.squeeze(self.v_mlp(observation), axis=1)\n",
        "    return pi, logp_pi, v\n",
        "\n",
        "  def _mlp_categorical_policy(self, observation, action):\n",
        "    act_dim = self.action_space.n\n",
        "    logits = self.pi_mlp(observation)\n",
        "    logp_all = tf.nn.log_softmax(logits)\n",
        "    pi = tfd.Categorical(logits).sample()  # pi is the next action\n",
        "    if action is not None:\n",
        "      action = tf.cast(action, tf.int32)\n",
        "      logp = tf.reduce_sum(tf.one_hot(action, act_dim, dtype=tf.float64) * logp_all, axis=1)\n",
        "    else:\n",
        "      logp = tf.reduce_sum(tf.one_hot(pi, act_dim, dtype=tf.float64) * logp_all, axis=1)\n",
        "    return pi, logp\n",
        "\n",
        "  def _mlp_gaussian_policy(self, observation, action):\n",
        "    mu = self.pi_mlp(observation)\n",
        "    std = tf.exp(self.pi_mlp.log_std)\n",
        "    pi = mu + tf.random.normal(tf.shape(mu), dtype=tf.float64) * std  # pi is the next action\n",
        "    if action is not None: \n",
        "      action = tf.cast(action, tf.float64)\n",
        "      logp = gaussian_likelihood(action, mu, self.pi_mlp.log_std)\n",
        "    else:\n",
        "      logp = gaussian_likelihood(pi, mu, self.pi_mlp.log_std)\n",
        "    return pi, logp\n",
        "\n",
        "  def update(self, buf):\n",
        "    obs_buf, act_buf, adv_buf, ret_buf, logp_buf = buf.get()\n",
        "\n",
        "    with tf.GradientTape() as pi_tape, tf.GradientTape() as vf_tape:\n",
        "      pi, logp, v = self.__call__(obs_buf, act_buf)\n",
        "\n",
        "      pi_loss = -tf.reduce_mean(logp * adv_buf)\n",
        "      v_loss = tf.reduce_mean((ret_buf - v) ** 2)\n",
        "\n",
        "    pi_grads = pi_tape.gradient(pi_loss, self.pi_mlp.trainable_variables)\n",
        "    self.pi_optimizer.apply_gradients(zip(pi_grads, self.pi_mlp.trainable_variables))\n",
        "\n",
        "    vf_grads = vf_tape.gradient(v_loss, self.v_mlp.trainable_variables)\n",
        "    for _ in range(self.train_v_iters):\n",
        "      self.vf_optimizer.apply_gradients(zip(vf_grads, self.v_mlp.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UK_GSAyG8zP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VPGBuffer:\n",
        "  \"\"\"\n",
        "  A buffer for storing trajectories experienced by a VPG agent interacting\n",
        "  with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
        "  for calculating the advantages of state-action pairs.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
        "    self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
        "    self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
        "    self.adv_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.rew_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.ret_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.val_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.logp_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.gamma, self.lam = gamma, lam\n",
        "    self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
        "\n",
        "  def store(self, obs, act, rew, val, logp):\n",
        "    \"\"\"\n",
        "    Append one timestep of agent-environment interaction to the buffer.\n",
        "    \"\"\"\n",
        "    assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
        "    self.obs_buf[self.ptr] = obs\n",
        "    self.act_buf[self.ptr] = act\n",
        "    self.rew_buf[self.ptr] = rew\n",
        "    self.val_buf[self.ptr] = val\n",
        "    self.logp_buf[self.ptr] = logp\n",
        "    self.ptr += 1\n",
        "\n",
        "  def finish_path(self, last_val=0):\n",
        "    \"\"\"\n",
        "    Call this at the end of a trajectory, or when one gets cut off\n",
        "    by an epoch ending. This looks back in the buffer to where the\n",
        "    trajectory started, and uses rewards and value estimates from\n",
        "    the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
        "    as well as compute the rewards-to-go for each state, to use as\n",
        "    the targets for the value function.\n",
        "    The \"last_val\" argument should be 0 if the trajectory ended\n",
        "    because the agent reached a terminal state (died), and otherwise\n",
        "    should be V(s_T), the value function estimated for the last state.\n",
        "    This allows us to bootstrap the reward-to-go calculation to account\n",
        "    for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
        "    \"\"\"\n",
        "\n",
        "    path_slice = slice(self.path_start_idx, self.ptr)\n",
        "    rews = np.append(self.rew_buf[path_slice], last_val)\n",
        "    vals = np.append(self.val_buf[path_slice], last_val)\n",
        "\n",
        "    # the next two lines implement GAE-Lambda advantage calculation\n",
        "    deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
        "    self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
        "\n",
        "    # the next line computes rewards-to-go, to be targets for the value function\n",
        "    self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
        "\n",
        "    self.path_start_idx = self.ptr\n",
        "\n",
        "  def get(self):\n",
        "    \"\"\"\n",
        "    Call this at the end of an epoch to get all of the data from\n",
        "    the buffer, with advantages appropriately normalized (shifted to have\n",
        "    mean zero and std one). Also, resets some pointers in the buffer.\n",
        "    \"\"\"\n",
        "    assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
        "    self.ptr, self.path_start_idx = 0, 0\n",
        "    # the next two lines implement the advantage normalization trick\n",
        "    adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
        "    self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
        "    return self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, self.logp_buf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cez9cWKy8zzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Vanilla Policy Gradient\n",
        "(with GAE-Lambda for advantage estimation)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def vpg(env, ac_kwargs=None, seed=0, steps_per_epoch=4000, epochs=50, gamma=0.99, lam=0.97, max_ep_len=1000, save_freq=10):\n",
        "\n",
        "  tf.random.set_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  # Create actor-critic agent \n",
        "  ac_kwargs['action_space'] = env.action_space\n",
        "\n",
        "  actor_critic = ActorCritic(**ac_kwargs)\n",
        "\n",
        "\n",
        "  # Experience buffer\n",
        "  obs_dim = env.observation_space.shape\n",
        "  act_dim = env.action_space.shape\n",
        "  buf = VPGBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
        "\n",
        "  \"\"\"\n",
        "  Main loop: collect experience in env and update/log each epoch\n",
        "  \"\"\"\n",
        "\n",
        "  # o for observation, r for reward, d for done\n",
        "  o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "\n",
        "  all_ep_ret = []\n",
        "  summary_ep_ret = []\n",
        "  totalEnvInteracts = []\n",
        "  for epoch in range(epochs):\n",
        "    for t in range(steps_per_epoch):\n",
        "      a, logp_t, v_t = actor_critic(o.reshape(1, -1), None)\n",
        "\n",
        "      # save and log\n",
        "      a = a.numpy()[0]\n",
        "      buf.store(o, a, r, v_t, logp_t)\n",
        "\n",
        "      o, r, d, _ = env.step(a)\n",
        "      ep_ret += r\n",
        "      ep_len += 1\n",
        "\n",
        "      terminal = d or (ep_len == max_ep_len)\n",
        "      if terminal or (t == steps_per_epoch - 1):\n",
        "        if not(terminal):\n",
        "          print('Warning: trajectory cut off by epoch at %d steps.' % ep_len)\n",
        "        # if trajectory didn't reach terminal state, bootstrap value target\n",
        "        last_val = r if d else v_t\n",
        "        buf.finish_path(last_val)\n",
        "\n",
        "        if terminal:\n",
        "          all_ep_ret.append(ep_ret)\n",
        "        # reset environment\n",
        "        o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "\n",
        "    # Perform VPG update!\n",
        "    actor_critic.update(buf)\n",
        "    mean, std = statistics_scalar(all_ep_ret)\n",
        "    all_ep_ret = []\n",
        "\n",
        "    print(f'epoch {epoch}: mean {mean}, std {std}')\n",
        "    summary_ep_ret.append(mean)\n",
        "    totalEnvInteracts.append((epoch + 1) * steps_per_epoch)\n",
        "\n",
        "  plt.plot(totalEnvInteracts, summary_ep_ret)\n",
        "  plt.grid(True)\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xVwDQk7b81n4",
        "colab_type": "code",
        "outputId": "6a0dd41e-fc11-46f6-8e48-f2cc1a1a65f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2711
        }
      },
      "cell_type": "code",
      "source": [
        "vpg(gym.make('CartPole-v0'), ac_kwargs=dict(hidden_sizes=[64] * 2), gamma=0.99, seed=0, steps_per_epoch=4000, epochs=70)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: trajectory cut off by epoch at 14 steps.\n",
            "epoch 0: mean 20.5463924407959, std 9.774805068969727\n",
            "Warning: trajectory cut off by epoch at 33 steps.\n",
            "epoch 1: mean 21.559782028198242, std 11.016129493713379\n",
            "Warning: trajectory cut off by epoch at 39 steps.\n",
            "epoch 2: mean 22.895954132080078, std 12.354618072509766\n",
            "Warning: trajectory cut off by epoch at 8 steps.\n",
            "epoch 3: mean 23.904191970825195, std 13.226825714111328\n",
            "Warning: trajectory cut off by epoch at 20 steps.\n",
            "epoch 4: mean 23.41176414489746, std 12.919929504394531\n",
            "Warning: trajectory cut off by epoch at 10 steps.\n",
            "epoch 5: mean 23.47058868408203, std 10.895318031311035\n",
            "Warning: trajectory cut off by epoch at 15 steps.\n",
            "epoch 6: mean 23.304094314575195, std 12.470359802246094\n",
            "Warning: trajectory cut off by epoch at 6 steps.\n",
            "epoch 7: mean 26.104576110839844, std 14.425904273986816\n",
            "Warning: trajectory cut off by epoch at 14 steps.\n",
            "epoch 8: mean 27.115646362304688, std 11.459196090698242\n",
            "Warning: trajectory cut off by epoch at 10 steps.\n",
            "epoch 9: mean 27.328767776489258, std 14.900634765625\n",
            "Warning: trajectory cut off by epoch at 37 steps.\n",
            "epoch 10: mean 27.33103370666504, std 14.999336242675781\n",
            "Warning: trajectory cut off by epoch at 18 steps.\n",
            "epoch 11: mean 28.042253494262695, std 15.510047912597656\n",
            "Warning: trajectory cut off by epoch at 7 steps.\n",
            "epoch 12: mean 29.145984649658203, std 15.22509479522705\n",
            "Warning: trajectory cut off by epoch at 21 steps.\n",
            "epoch 13: mean 30.60769271850586, std 16.07219123840332\n",
            "Warning: trajectory cut off by epoch at 44 steps.\n",
            "epoch 14: mean 32.6942138671875, std 17.16514015197754\n",
            "Warning: trajectory cut off by epoch at 16 steps.\n",
            "epoch 15: mean 32.925621032714844, std 17.599853515625\n",
            "Warning: trajectory cut off by epoch at 80 steps.\n",
            "epoch 16: mean 34.08695602416992, std 19.43710708618164\n",
            "Warning: trajectory cut off by epoch at 9 steps.\n",
            "epoch 17: mean 34.11111068725586, std 18.25965690612793\n",
            "Warning: trajectory cut off by epoch at 8 steps.\n",
            "epoch 18: mean 31.682538986206055, std 15.489192962646484\n",
            "Warning: trajectory cut off by epoch at 11 steps.\n",
            "epoch 19: mean 36.59632873535156, std 18.667600631713867\n",
            "Warning: trajectory cut off by epoch at 25 steps.\n",
            "epoch 20: mean 34.565216064453125, std 21.59393310546875\n",
            "Warning: trajectory cut off by epoch at 10 steps.\n",
            "epoch 21: mean 35.945945739746094, std 16.529438018798828\n",
            "Warning: trajectory cut off by epoch at 25 steps.\n",
            "epoch 22: mean 39.3564338684082, std 22.579490661621094\n",
            "Warning: trajectory cut off by epoch at 13 steps.\n",
            "epoch 23: mean 36.91666793823242, std 16.09182357788086\n",
            "Warning: trajectory cut off by epoch at 28 steps.\n",
            "epoch 24: mean 37.47169876098633, std 18.96630859375\n",
            "Warning: trajectory cut off by epoch at 52 steps.\n",
            "epoch 25: mean 40.28571319580078, std 21.340770721435547\n",
            "Warning: trajectory cut off by epoch at 25 steps.\n",
            "epoch 26: mean 38.59223175048828, std 17.340919494628906\n",
            "Warning: trajectory cut off by epoch at 3 steps.\n",
            "epoch 27: mean 40.78571319580078, std 20.747879028320312\n",
            "Warning: trajectory cut off by epoch at 2 steps.\n",
            "epoch 28: mean 38.81553268432617, std 17.168354034423828\n",
            "epoch 29: mean 43.0107536315918, std 18.985843658447266\n",
            "Warning: trajectory cut off by epoch at 20 steps.\n",
            "epoch 30: mean 40.61224365234375, std 20.212459564208984\n",
            "Warning: trajectory cut off by epoch at 5 steps.\n",
            "epoch 31: mean 42.5, std 23.769617080688477\n",
            "Warning: trajectory cut off by epoch at 29 steps.\n",
            "epoch 32: mean 44.617977142333984, std 21.885046005249023\n",
            "epoch 33: mean 43.0107536315918, std 20.52876091003418\n",
            "Warning: trajectory cut off by epoch at 58 steps.\n",
            "epoch 34: mean 46.92856979370117, std 26.64394187927246\n",
            "Warning: trajectory cut off by epoch at 43 steps.\n",
            "epoch 35: mean 47.67469787597656, std 25.88115119934082\n",
            "Warning: trajectory cut off by epoch at 55 steps.\n",
            "epoch 36: mean 46.96428680419922, std 25.401277542114258\n",
            "Warning: trajectory cut off by epoch at 9 steps.\n",
            "epoch 37: mean 45.352272033691406, std 23.477764129638672\n",
            "Warning: trajectory cut off by epoch at 52 steps.\n",
            "epoch 38: mean 48.74074172973633, std 25.203262329101562\n",
            "Warning: trajectory cut off by epoch at 94 steps.\n",
            "epoch 39: mean 48.22222137451172, std 20.53963279724121\n",
            "Warning: trajectory cut off by epoch at 13 steps.\n",
            "epoch 40: mean 45.82758712768555, std 20.793018341064453\n",
            "Warning: trajectory cut off by epoch at 58 steps.\n",
            "epoch 41: mean 50.53845977783203, std 27.335376739501953\n",
            "Warning: trajectory cut off by epoch at 28 steps.\n",
            "epoch 42: mean 51.584415435791016, std 27.57579803466797\n",
            "Warning: trajectory cut off by epoch at 44 steps.\n",
            "epoch 43: mean 47.095237731933594, std 18.814647674560547\n",
            "Warning: trajectory cut off by epoch at 6 steps.\n",
            "epoch 44: mean 51.20512771606445, std 24.60631561279297\n",
            "Warning: trajectory cut off by epoch at 16 steps.\n",
            "epoch 45: mean 51.74026107788086, std 32.5601921081543\n",
            "Warning: trajectory cut off by epoch at 22 steps.\n",
            "epoch 46: mean 55.25, std 28.39588737487793\n",
            "Warning: trajectory cut off by epoch at 3 steps.\n",
            "epoch 47: mean 57.92753601074219, std 27.17379379272461\n",
            "Warning: trajectory cut off by epoch at 3 steps.\n",
            "epoch 48: mean 50.59493637084961, std 23.769737243652344\n",
            "Warning: trajectory cut off by epoch at 24 steps.\n",
            "epoch 49: mean 53.01333236694336, std 23.890579223632812\n",
            "Warning: trajectory cut off by epoch at 28 steps.\n",
            "epoch 50: mean 60.181819915771484, std 23.858108520507812\n",
            "Warning: trajectory cut off by epoch at 55 steps.\n",
            "epoch 51: mean 58.88059616088867, std 28.80221939086914\n",
            "Warning: trajectory cut off by epoch at 2 steps.\n",
            "epoch 52: mean 54.76712417602539, std 25.115907669067383\n",
            "Warning: trajectory cut off by epoch at 13 steps.\n",
            "epoch 53: mean 57.78260803222656, std 30.259204864501953\n",
            "Warning: trajectory cut off by epoch at 27 steps.\n",
            "epoch 54: mean 58.42647171020508, std 26.8975830078125\n",
            "Warning: trajectory cut off by epoch at 34 steps.\n",
            "epoch 55: mean 63.967742919921875, std 27.885692596435547\n",
            "Warning: trajectory cut off by epoch at 40 steps.\n",
            "epoch 56: mean 61.875, std 30.32918357849121\n",
            "Warning: trajectory cut off by epoch at 18 steps.\n",
            "epoch 57: mean 65.2786865234375, std 33.81973648071289\n",
            "Warning: trajectory cut off by epoch at 31 steps.\n",
            "epoch 58: mean 62.015625, std 31.875974655151367\n",
            "Warning: trajectory cut off by epoch at 28 steps.\n",
            "epoch 59: mean 67.32203674316406, std 36.081565856933594\n",
            "Warning: trajectory cut off by epoch at 71 steps.\n",
            "epoch 60: mean 70.16071319580078, std 32.6019172668457\n",
            "Warning: trajectory cut off by epoch at 36 steps.\n",
            "epoch 61: mean 70.78571319580078, std 32.70305633544922\n",
            "Warning: trajectory cut off by epoch at 24 steps.\n",
            "epoch 62: mean 71.0, std 41.58382034301758\n",
            "Warning: trajectory cut off by epoch at 49 steps.\n",
            "epoch 63: mean 74.54717254638672, std 40.430030822753906\n",
            "Warning: trajectory cut off by epoch at 11 steps.\n",
            "epoch 64: mean 78.21568298339844, std 39.417476654052734\n",
            "Warning: trajectory cut off by epoch at 25 steps.\n",
            "epoch 65: mean 84.57447052001953, std 46.54658889770508\n",
            "Warning: trajectory cut off by epoch at 49 steps.\n",
            "epoch 66: mean 84.06382751464844, std 43.01676940917969\n",
            "Warning: trajectory cut off by epoch at 34 steps.\n",
            "epoch 67: mean 88.13333129882812, std 47.9761962890625\n",
            "Warning: trajectory cut off by epoch at 21 steps.\n",
            "epoch 68: mean 97.04878234863281, std 49.329627990722656\n",
            "Warning: trajectory cut off by epoch at 87 steps.\n",
            "epoch 69: mean 91.0, std 43.5233154296875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npZRyc5rolVLKzf1/d11gfxkp2ygAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}