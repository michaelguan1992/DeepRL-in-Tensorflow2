{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vpg.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelguan1992/spinningup-in-deeprl-tensorflow2/blob/master/algos/vpg_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2kbrJGOD8aha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow==2.0.0-alpha0\n",
        "!pip install -q tfp-nightly\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import gym\n",
        "from gym.spaces import Box, Discrete\n",
        "import time\n",
        "\n",
        "import scipy.signal\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OYL5HDm_8s_Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "functions in core.py\n",
        "\"\"\"\n",
        "EPS = 1e-8\n",
        "\n",
        "\n",
        "def combined_shape(length, shape=None):\n",
        "  if shape is None:\n",
        "    return (length,)\n",
        "  return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
        "\n",
        "\n",
        "def discount_cumsum(x, discount):\n",
        "  \"\"\"\n",
        "  magic from rllab for computing discounted cumulative sums of vectors.\n",
        "  input:\n",
        "    vector x,\n",
        "    [x0,\n",
        "     x1,\n",
        "     x2]\n",
        "  output:\n",
        "    [x0 + discount * x1 + discount^2 * x2,\n",
        "     x1 + discount * x2,\n",
        "     x2]\n",
        "  \"\"\"\n",
        "  return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "def gaussian_likelihood(x, mu, log_std):\n",
        "  pre_sum = -0.5 * (((x - mu) / (tf.exp(log_std) + EPS))**2 + 2 * log_std + np.log(2 * np.pi))\n",
        "  return tf.reduce_sum(pre_sum, axis=1)\n",
        "  \n",
        "class MLP(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               sizes, \n",
        "               activation='tanh', \n",
        "               output_activation=None,\n",
        "               is_continue_action=False,\n",
        "               act_dim=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.denses = [tf.keras.layers.Dense(size, activation=activation) for size in sizes[:-1]]\n",
        "    self.out = tf.keras.layers.Dense(sizes[-1], activation=output_activation)\n",
        "    if is_continue_action:\n",
        "      if act_dim is None:\n",
        "        raise TypeError(\"__init__() missing 1 argument: 'act_dim' when log_std=True\")\n",
        "      self.log_std = tf.Variable(name='log_std', initial_value=-0.5 * np.ones(act_dim, dtype=np.float32))\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    for dense in self.denses:\n",
        "      x = dense(x)      \n",
        "    return self.out(x)\n",
        "\n",
        "def statistics_scalar(x):\n",
        "  x = np.array(x, dtype=np.float32)\n",
        "  return np.mean(x), np.std(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "59XaWHsg8u2N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Actor-Critics\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ActorCritic:\n",
        "  def __init__(self, hidden_sizes=(64, 64), activation='tanh', output_activation=None, action_space=None, pi_lr=3e-4, vf_lr=1e-3, train_v_iters=80):\n",
        "    if isinstance(action_space, Box):\n",
        "      act_dim = len(action_space.sample())\n",
        "      self.pi_mlp = MLP(list(hidden_sizes) + [act_dim], activation, output_activation, is_continue_action=True, act_dim=act_dim)\n",
        "      self.policy = self._mlp_gaussian_policy\n",
        "\n",
        "    elif isinstance(action_space, Discrete):\n",
        "      act_dim = action_space.n\n",
        "      self.pi_mlp = MLP(list(hidden_sizes) + [act_dim], activation, None)\n",
        "      self.policy = self._mlp_categorical_policy      \n",
        "      self.action_space = action_space\n",
        "\n",
        "    self.v_mlp = MLP(list(hidden_sizes) + [1], activation, None)\n",
        "    \n",
        "    # optimizers\n",
        "    self.pi_optimizer = tf.optimizers.Adam(learning_rate=pi_lr)\n",
        "    self.vf_optimizer = tf.optimizers.Adam(learning_rate=vf_lr)\n",
        "    self.train_v_iters = train_v_iters\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, observation, action):\n",
        "    pi, logp_pi = self.policy(observation, action)\n",
        "    v = tf.squeeze(self.v_mlp(observation), axis=1)\n",
        "    return pi, logp_pi, v\n",
        "\n",
        "  def _mlp_categorical_policy(self, observation, action):\n",
        "    act_dim = self.action_space.n\n",
        "    logits = self.pi_mlp(observation)\n",
        "    logp_all = tf.nn.log_softmax(logits)\n",
        "    pi = tfd.Categorical(logits).sample()  # pi is the next action\n",
        "    if action is not None:\n",
        "      action = tf.cast(action, tf.int32)\n",
        "      logp = tf.reduce_sum(tf.one_hot(action, act_dim) * logp_all, axis=1)\n",
        "    else:\n",
        "      logp = tf.reduce_sum(tf.one_hot(pi, act_dim) * logp_all, axis=1)\n",
        "    return pi, logp\n",
        "\n",
        "  def _mlp_gaussian_policy(self, observation, action):\n",
        "    mu = self.pi_mlp(observation)\n",
        "    std = tf.exp(self.pi_mlp.log_std)\n",
        "    pi = mu + tf.random.normal(tf.shape(mu)) * std  # pi is the next action\n",
        "    if action is not None: \n",
        "      logp = gaussian_likelihood(action, mu, self.pi_mlp.log_std)\n",
        "    else:\n",
        "      logp = gaussian_likelihood(pi, mu, self.pi_mlp.log_std)\n",
        "    return pi, logp\n",
        "  \n",
        "  def update(self, buf):\n",
        "    obs_buf, act_buf, adv_buf, ret_buf, logp_buf = buf.get()\n",
        "\n",
        "    with tf.GradientTape() as pi_tape:\n",
        "      pi, logp, v = self(obs_buf, act_buf)\n",
        "      pi_loss = -tf.reduce_mean(logp * adv_buf)      \n",
        "\n",
        "    pi_grads = pi_tape.gradient(pi_loss, self.pi_mlp.trainable_variables)\n",
        "    self.pi_optimizer.apply_gradients(zip(pi_grads, self.pi_mlp.trainable_variables))\n",
        "  \n",
        "    for _ in range(self.train_v_iters):\n",
        "      with tf.GradientTape() as vf_tape:\n",
        "        pi, logp, v = self(obs_buf, act_buf)\n",
        "        v_loss = tf.reduce_mean((ret_buf - v) ** 2)\n",
        "      vf_grads = vf_tape.gradient(v_loss, self.v_mlp.trainable_variables)\n",
        "      self.vf_optimizer.apply_gradients(zip(vf_grads, self.v_mlp.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UK_GSAyG8zP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VPGBuffer:\n",
        "  \"\"\"\n",
        "  A buffer for storing trajectories experienced by a VPG agent interacting\n",
        "  with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
        "  for calculating the advantages of state-action pairs.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
        "    self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
        "    self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
        "    self.adv_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.rew_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.ret_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.val_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.logp_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.gamma, self.lam = gamma, lam\n",
        "    self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
        "\n",
        "  def store(self, obs, act, rew, val, logp):\n",
        "    \"\"\"\n",
        "    Append one timestep of agent-environment interaction to the buffer.\n",
        "    \"\"\"\n",
        "    assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
        "    self.obs_buf[self.ptr] = obs\n",
        "    self.act_buf[self.ptr] = act\n",
        "    self.rew_buf[self.ptr] = rew\n",
        "    self.val_buf[self.ptr] = val\n",
        "    self.logp_buf[self.ptr] = logp\n",
        "    self.ptr += 1\n",
        "\n",
        "  def finish_path(self, last_val=0):\n",
        "    \"\"\"\n",
        "    Call this at the end of a trajectory, or when one gets cut off\n",
        "    by an epoch ending. This looks back in the buffer to where the\n",
        "    trajectory started, and uses rewards and value estimates from\n",
        "    the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
        "    as well as compute the rewards-to-go for each state, to use as\n",
        "    the targets for the value function.\n",
        "    The \"last_val\" argument should be 0 if the trajectory ended\n",
        "    because the agent reached a terminal state (died), and otherwise\n",
        "    should be V(s_T), the value function estimated for the last state.\n",
        "    This allows us to bootstrap the reward-to-go calculation to account\n",
        "    for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
        "    \"\"\"\n",
        "\n",
        "    path_slice = slice(self.path_start_idx, self.ptr)\n",
        "    rews = np.append(self.rew_buf[path_slice], last_val)\n",
        "    vals = np.append(self.val_buf[path_slice], last_val)\n",
        "\n",
        "    # the next two lines implement GAE-Lambda advantage calculation\n",
        "    deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
        "    self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
        "\n",
        "    # the next line computes rewards-to-go, to be targets for the value function\n",
        "    self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
        "\n",
        "    self.path_start_idx = self.ptr\n",
        "\n",
        "  def get(self):\n",
        "    \"\"\"\n",
        "    Call this at the end of an epoch to get all of the data from\n",
        "    the buffer, with advantages appropriately normalized (shifted to have\n",
        "    mean zero and std one). Also, resets some pointers in the buffer.\n",
        "    \"\"\"\n",
        "    assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
        "    self.ptr, self.path_start_idx = 0, 0\n",
        "    # the next two lines implement the advantage normalization trick\n",
        "    adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
        "    self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
        "    return self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, self.logp_buf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cez9cWKy8zzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Vanilla Policy Gradient\n",
        "(with GAE-Lambda for advantage estimation)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def vpg(env, ac_kwargs=None, seed=0, steps_per_epoch=4000, epochs=50, gamma=0.99, lam=0.97, max_ep_len=1000, save_freq=10):\n",
        "\n",
        "  tf.random.set_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  # Create actor-critic agent \n",
        "  ac_kwargs['action_space'] = env.action_space\n",
        "\n",
        "  actor_critic = ActorCritic(**ac_kwargs)\n",
        "\n",
        "\n",
        "  # Experience buffer\n",
        "  obs_dim = env.observation_space.shape\n",
        "  act_dim = env.action_space.shape\n",
        "  buf = VPGBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
        "\n",
        "  \"\"\"\n",
        "  Main loop: collect experience in env and update/log each epoch\n",
        "  \"\"\"\n",
        "\n",
        "  # o for observation, r for reward, d for done\n",
        "  o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "\n",
        "  all_ep_ret = []\n",
        "  summary_ep_ret = []\n",
        "  totalEnvInteracts = []\n",
        "  for epoch in range(epochs):\n",
        "    for t in range(steps_per_epoch):\n",
        "      a, logp_t, v_t = actor_critic(o.reshape(1, -1).astype(np.float32), None)\n",
        "\n",
        "      # save and log\n",
        "      a = a.numpy()[0]\n",
        "      buf.store(o, a, r, v_t, logp_t)\n",
        "\n",
        "      o, r, d, _ = env.step(a)\n",
        "      ep_ret += r\n",
        "      ep_len += 1\n",
        "\n",
        "      terminal = d or (ep_len == max_ep_len)\n",
        "      if terminal or (t == steps_per_epoch - 1):\n",
        "        if not(terminal):\n",
        "          print('Warning: trajectory cut off by epoch at %d steps.' % ep_len)\n",
        "        # if trajectory didn't reach terminal state, bootstrap value target\n",
        "        last_val = r if d else v_t\n",
        "        buf.finish_path(last_val)\n",
        "\n",
        "        if terminal:\n",
        "          all_ep_ret.append(ep_ret)\n",
        "        # reset environment\n",
        "        o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "\n",
        "    # Perform VPG update!\n",
        "    actor_critic.update(buf)\n",
        "    mean, std = statistics_scalar(all_ep_ret)\n",
        "    all_ep_ret = []\n",
        "\n",
        "    print(f'epoch {epoch}: mean {mean}, std {std}')\n",
        "    summary_ep_ret.append(mean)\n",
        "    totalEnvInteracts.append((epoch + 1) * steps_per_epoch)\n",
        "\n",
        "  plt.plot(totalEnvInteracts, summary_ep_ret)\n",
        "  plt.grid(True)\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xVwDQk7b81n4",
        "colab_type": "code",
        "outputId": "3db5c19d-c324-42b8-e698-62b4ba51abf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2711
        }
      },
      "cell_type": "code",
      "source": [
        "vpg(gym.make('CartPole-v0'), ac_kwargs=dict(hidden_sizes=[64] * 2), gamma=0.99, seed=0, steps_per_epoch=4000, epochs=70)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: trajectory cut off by epoch at 14 steps.\n",
            "epoch 0: mean 16.747900009155273, std 6.916936874389648\n",
            "Warning: trajectory cut off by epoch at 13 steps.\n",
            "epoch 1: mean 17.259740829467773, std 7.557690143585205\n",
            "Warning: trajectory cut off by epoch at 7 steps.\n",
            "epoch 2: mean 18.23287582397461, std 8.550981521606445\n",
            "Warning: trajectory cut off by epoch at 1 steps.\n",
            "epoch 3: mean 18.952606201171875, std 7.929047584533691\n",
            "Warning: trajectory cut off by epoch at 4 steps.\n",
            "epoch 4: mean 20.18181800842285, std 10.268972396850586\n",
            "Warning: trajectory cut off by epoch at 25 steps.\n",
            "epoch 5: mean 19.01913833618164, std 8.423572540283203\n",
            "Warning: trajectory cut off by epoch at 8 steps.\n",
            "epoch 6: mean 19.47317123413086, std 9.585838317871094\n",
            "epoch 7: mean 21.85792350769043, std 11.683977127075195\n",
            "Warning: trajectory cut off by epoch at 8 steps.\n",
            "epoch 8: mean 21.347593307495117, std 11.105331420898438\n",
            "Warning: trajectory cut off by epoch at 6 steps.\n",
            "epoch 9: mean 20.910995483398438, std 10.027054786682129\n",
            "Warning: trajectory cut off by epoch at 30 steps.\n",
            "epoch 10: mean 23.7724552154541, std 13.83853816986084\n",
            "Warning: trajectory cut off by epoch at 40 steps.\n",
            "epoch 11: mean 23.023256301879883, std 12.14206314086914\n",
            "Warning: trajectory cut off by epoch at 6 steps.\n",
            "epoch 12: mean 24.503067016601562, std 13.936038970947266\n",
            "Warning: trajectory cut off by epoch at 21 steps.\n",
            "epoch 13: mean 23.969879150390625, std 15.97830867767334\n",
            "Warning: trajectory cut off by epoch at 12 steps.\n",
            "epoch 14: mean 26.065359115600586, std 16.940753936767578\n",
            "Warning: trajectory cut off by epoch at 7 steps.\n",
            "epoch 15: mean 27.537931442260742, std 18.201183319091797\n",
            "Warning: trajectory cut off by epoch at 14 steps.\n",
            "epoch 16: mean 27.115646362304688, std 14.155365943908691\n",
            "epoch 17: mean 29.850746154785156, std 17.676406860351562\n",
            "Warning: trajectory cut off by epoch at 44 steps.\n",
            "epoch 18: mean 31.396825790405273, std 20.974315643310547\n",
            "Warning: trajectory cut off by epoch at 41 steps.\n",
            "epoch 19: mean 34.72806930541992, std 20.80118179321289\n",
            "Warning: trajectory cut off by epoch at 15 steps.\n",
            "epoch 20: mean 33.487396240234375, std 19.936397552490234\n",
            "Warning: trajectory cut off by epoch at 5 steps.\n",
            "epoch 21: mean 37.68867874145508, std 23.409948348999023\n",
            "Warning: trajectory cut off by epoch at 23 steps.\n",
            "epoch 22: mean 36.48624038696289, std 24.586345672607422\n",
            "Warning: trajectory cut off by epoch at 16 steps.\n",
            "epoch 23: mean 35.25663757324219, std 25.373939514160156\n",
            "Warning: trajectory cut off by epoch at 26 steps.\n",
            "epoch 24: mean 38.960784912109375, std 21.581092834472656\n",
            "Warning: trajectory cut off by epoch at 19 steps.\n",
            "epoch 25: mean 40.6224479675293, std 21.159509658813477\n",
            "Warning: trajectory cut off by epoch at 47 steps.\n",
            "epoch 26: mean 47.62650680541992, std 32.487144470214844\n",
            "Warning: trajectory cut off by epoch at 3 steps.\n",
            "epoch 27: mean 43.44565200805664, std 27.46197509765625\n",
            "Warning: trajectory cut off by epoch at 25 steps.\n",
            "epoch 28: mean 44.16666793823242, std 33.54838943481445\n",
            "Warning: trajectory cut off by epoch at 31 steps.\n",
            "epoch 29: mean 49.0, std 29.045303344726562\n",
            "Warning: trajectory cut off by epoch at 7 steps.\n",
            "epoch 30: mean 48.69512176513672, std 27.478012084960938\n",
            "Warning: trajectory cut off by epoch at 26 steps.\n",
            "epoch 31: mean 46.20930099487305, std 30.071664810180664\n",
            "Warning: trajectory cut off by epoch at 73 steps.\n",
            "epoch 32: mean 48.48147964477539, std 27.579788208007812\n",
            "Warning: trajectory cut off by epoch at 93 steps.\n",
            "epoch 33: mean 48.837501525878906, std 27.8435115814209\n",
            "Warning: trajectory cut off by epoch at 37 steps.\n",
            "epoch 34: mean 55.04166793823242, std 31.854894638061523\n",
            "Warning: trajectory cut off by epoch at 2 steps.\n",
            "epoch 35: mean 57.9420280456543, std 39.1325798034668\n",
            "Warning: trajectory cut off by epoch at 105 steps.\n",
            "epoch 36: mean 54.859153747558594, std 35.463645935058594\n",
            "Warning: trajectory cut off by epoch at 19 steps.\n",
            "epoch 37: mean 61.24615478515625, std 31.700048446655273\n",
            "Warning: trajectory cut off by epoch at 26 steps.\n",
            "epoch 38: mean 57.59420394897461, std 35.318145751953125\n",
            "Warning: trajectory cut off by epoch at 59 steps.\n",
            "epoch 39: mean 59.712120056152344, std 32.610076904296875\n",
            "Warning: trajectory cut off by epoch at 139 steps.\n",
            "epoch 40: mean 57.62686538696289, std 38.629058837890625\n",
            "Warning: trajectory cut off by epoch at 67 steps.\n",
            "epoch 41: mean 56.18571472167969, std 35.96200180053711\n",
            "Warning: trajectory cut off by epoch at 36 steps.\n",
            "epoch 42: mean 63.935482025146484, std 34.60182189941406\n",
            "Warning: trajectory cut off by epoch at 49 steps.\n",
            "epoch 43: mean 58.970149993896484, std 29.820842742919922\n",
            "Warning: trajectory cut off by epoch at 72 steps.\n",
            "epoch 44: mean 66.5762710571289, std 41.21757125854492\n",
            "Warning: trajectory cut off by epoch at 46 steps.\n",
            "epoch 45: mean 68.17241668701172, std 41.98815155029297\n",
            "Warning: trajectory cut off by epoch at 50 steps.\n",
            "epoch 46: mean 60.769229888916016, std 35.25501251220703\n",
            "Warning: trajectory cut off by epoch at 50 steps.\n",
            "epoch 47: mean 71.81818389892578, std 36.58270263671875\n",
            "Warning: trajectory cut off by epoch at 26 steps.\n",
            "epoch 48: mean 73.59259033203125, std 39.887115478515625\n",
            "Warning: trajectory cut off by epoch at 181 steps.\n",
            "epoch 49: mean 69.43636322021484, std 39.39255142211914\n",
            "Warning: trajectory cut off by epoch at 11 steps.\n",
            "epoch 50: mean 79.77999877929688, std 48.446380615234375\n",
            "Warning: trajectory cut off by epoch at 24 steps.\n",
            "epoch 51: mean 77.96078491210938, std 40.43729782104492\n",
            "Warning: trajectory cut off by epoch at 32 steps.\n",
            "epoch 52: mean 70.85713958740234, std 46.10447311401367\n",
            "Warning: trajectory cut off by epoch at 32 steps.\n",
            "epoch 53: mean 69.6140365600586, std 43.327701568603516\n",
            "Warning: trajectory cut off by epoch at 66 steps.\n",
            "epoch 54: mean 81.95833587646484, std 49.51933670043945\n",
            "Warning: trajectory cut off by epoch at 113 steps.\n",
            "epoch 55: mean 80.97916412353516, std 43.431217193603516\n",
            "Warning: trajectory cut off by epoch at 88 steps.\n",
            "epoch 56: mean 83.23403930664062, std 51.69395065307617\n",
            "Warning: trajectory cut off by epoch at 13 steps.\n",
            "epoch 57: mean 90.61363983154297, std 51.50428771972656\n",
            "Warning: trajectory cut off by epoch at 85 steps.\n",
            "epoch 58: mean 85.10869598388672, std 45.47298812866211\n",
            "Warning: trajectory cut off by epoch at 9 steps.\n",
            "epoch 59: mean 86.76087188720703, std 50.68795394897461\n",
            "Warning: trajectory cut off by epoch at 15 steps.\n",
            "epoch 60: mean 90.56818389892578, std 50.33451843261719\n",
            "Warning: trajectory cut off by epoch at 49 steps.\n",
            "epoch 61: mean 106.78378295898438, std 53.14596939086914\n",
            "Warning: trajectory cut off by epoch at 29 steps.\n",
            "epoch 62: mean 101.82051086425781, std 50.32859420776367\n",
            "Warning: trajectory cut off by epoch at 16 steps.\n",
            "epoch 63: mean 90.54545593261719, std 46.70091247558594\n",
            "Warning: trajectory cut off by epoch at 1 steps.\n",
            "epoch 64: mean 102.53845977783203, std 46.84608840942383\n",
            "Warning: trajectory cut off by epoch at 79 steps.\n",
            "epoch 65: mean 108.91666412353516, std 55.70276641845703\n",
            "Warning: trajectory cut off by epoch at 108 steps.\n",
            "epoch 66: mean 114.47058868408203, std 57.11713409423828\n",
            "Warning: trajectory cut off by epoch at 121 steps.\n",
            "epoch 67: mean 110.82857513427734, std 57.53035354614258\n",
            "Warning: trajectory cut off by epoch at 25 steps.\n",
            "epoch 68: mean 110.41666412353516, std 51.04158020019531\n",
            "Warning: trajectory cut off by epoch at 129 steps.\n",
            "epoch 69: mean 104.62162017822266, std 56.772830963134766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}