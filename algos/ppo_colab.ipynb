{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelguan1992/spinningup-in-deeprl-tensorflow2/blob/master/algos/ppo_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NzCWrr_frX9D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow==2.0.0-alpha0\n",
        "!pip install -q tfp-nightly\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import gym\n",
        "from gym.spaces import Box, Discrete\n",
        "import time\n",
        "\n",
        "import scipy.signal\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Eibf_f9-u8p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "functions in core.py\n",
        "\"\"\"\n",
        "EPS = 1e-8\n",
        "\n",
        "\n",
        "def combined_shape(length, shape=None):\n",
        "  if shape is None:\n",
        "    return (length,)\n",
        "  return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
        "\n",
        "\n",
        "def discount_cumsum(x, discount):\n",
        "  \"\"\"\n",
        "  magic from rllab for computing discounted cumulative sums of vectors.\n",
        "  input:\n",
        "    vector x,\n",
        "    [x0,\n",
        "     x1,\n",
        "     x2]\n",
        "  output:\n",
        "    [x0 + discount * x1 + discount^2 * x2,\n",
        "     x1 + discount * x2,\n",
        "     x2]\n",
        "  \"\"\"\n",
        "  return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "def gaussian_likelihood(x, mu, log_std):\n",
        "  pre_sum = -0.5 * (((x - mu) / (tf.exp(log_std) + EPS))**2 + 2 * log_std + np.log(2 * np.pi))\n",
        "  return tf.reduce_sum(pre_sum, axis=1)\n",
        "  \n",
        "class MLP(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               sizes, \n",
        "               activation='tanh', \n",
        "               output_activation=None,\n",
        "               is_continue_action=False,\n",
        "               act_dim=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.denses = [tf.keras.layers.Dense(size, activation=activation) for size in sizes[:-1]]\n",
        "    self.out = tf.keras.layers.Dense(sizes[-1], activation=output_activation)\n",
        "    if is_continue_action:\n",
        "      if act_dim is None:\n",
        "        raise TypeError(\"__init__() missing 1 argument: 'act_dim' when log_std=True\")\n",
        "      self.log_std = tf.Variable(name='log_std', initial_value=-0.5 * np.ones(act_dim, dtype=np.float32))\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    for dense in self.denses:\n",
        "      x = dense(x)      \n",
        "    return self.out(x)\n",
        "\n",
        "def statistics_scalar(x):\n",
        "  x = np.array(x, dtype=np.float32)\n",
        "  return np.mean(x), np.std(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ToON03yHEF2H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Actor-Critics\n",
        "\"\"\"\n",
        "\n",
        "class ActorCritic:\n",
        "  def __init__(self, hidden_sizes=(64, 64), activation='tanh',\n",
        "               output_activation=None, action_space=None, pi_lr=3e-4, vf_lr=1e-3,\n",
        "               train_v_iters=80, train_pi_iters=80):\n",
        "    if isinstance(action_space, Box):\n",
        "      act_dim = len(action_space.sample())\n",
        "      self.pi_mlp = MLP(list(hidden_sizes) + [act_dim], activation, output_activation, is_continue_action=True, act_dim=act_dim)\n",
        "      self.policy = self._mlp_gaussian_policy\n",
        "\n",
        "    elif isinstance(action_space, Discrete):\n",
        "      act_dim = action_space.n\n",
        "      self.pi_mlp = MLP(list(hidden_sizes) + [act_dim], activation, None)\n",
        "      self.policy = self._mlp_categorical_policy      \n",
        "      self.action_space = action_space\n",
        "      \n",
        "    # build mlp model\n",
        "    self.v_mlp = MLP(list(hidden_sizes) + [1], activation, None)\n",
        "    \n",
        "    # optimizers\n",
        "    self.pi_optimizer = tf.optimizers.Adam(learning_rate=pi_lr)\n",
        "    self.vf_optimizer = tf.optimizers.Adam(learning_rate=vf_lr)\n",
        "    self.train_v_iters = train_v_iters\n",
        "    self.train_pi_iters = train_pi_iters\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, observation, action):\n",
        "    pi, logp_pi = self.policy(observation, action)\n",
        "    v = tf.squeeze(self.v_mlp(observation), axis=1)\n",
        "    return pi, logp_pi, v\n",
        "\n",
        "  def _mlp_categorical_policy(self, observation, action):\n",
        "    act_dim = self.action_space.n\n",
        "    logits = self.pi_mlp(observation)\n",
        "    logp_all = tf.nn.log_softmax(logits)\n",
        "    pi = tfd.Categorical(logits).sample()  # pi is the next action\n",
        "    if action is not None:\n",
        "      action = tf.cast(action, tf.int32)\n",
        "      logp = tf.reduce_sum(tf.one_hot(action, act_dim) * logp_all, axis=1)\n",
        "    else:\n",
        "      logp = tf.reduce_sum(tf.one_hot(pi, act_dim) * logp_all, axis=1)\n",
        "    return pi, logp\n",
        "\n",
        "  def _mlp_gaussian_policy(self, observation, action):\n",
        "    mu = self.pi_mlp(observation)\n",
        "    std = tf.exp(self.pi_mlp.log_std)\n",
        "    pi = mu + tf.random.normal(tf.shape(mu)) * std  # pi is the next action\n",
        "    if action is not None: \n",
        "      logp = gaussian_likelihood(action, mu, self.pi_mlp.log_std)\n",
        "    else:\n",
        "      logp = gaussian_likelihood(pi, mu, self.pi_mlp.log_std)\n",
        "    return pi, logp\n",
        "\n",
        "  def update(self, buf, clip_ratio, target_kl):\n",
        "    obs_buf, act_buf, adv_buf, ret_buf, logp_buf = buf.get()\n",
        "\n",
        "    for i in range(self.train_pi_iters):\n",
        "      with tf.GradientTape() as pi_tape:\n",
        "        pi, logp, v = self(obs_buf, act_buf)\n",
        "        # PPO objectives\n",
        "        ratio = tf.exp(logp - logp_buf)\n",
        "        min_adv = tf.where(adv_buf>0, (1+clip_ratio)*adv_buf, (1-clip_ratio)*adv_buf)\n",
        "        # loss\n",
        "        pi_loss = -tf.reduce_mean(tf.minimum(ratio * adv_buf, min_adv))\n",
        "        \n",
        "      # Info (useful to watch during learning)\n",
        "      kl = tf.reduce_mean(logp_buf - logp) # a sample estimate for KL-divergence\n",
        "      if kl > 1.5 * target_kl:\n",
        "        print('Early stopping at step %d due to reaching max kl.' % i)\n",
        "        break\n",
        "\n",
        "      pi_grads = pi_tape.gradient(pi_loss, self.pi_mlp.trainable_variables)\n",
        "      self.pi_optimizer.apply_gradients(zip(pi_grads, self.pi_mlp.trainable_variables))\n",
        "\n",
        "    for _ in range(self.train_v_iters):\n",
        "      with tf.GradientTape() as vf_tape:\n",
        "        pi, logp, v = self(obs_buf, act_buf)\n",
        "        v_loss = tf.reduce_mean((ret_buf - v) ** 2)\n",
        "      vf_grads = vf_tape.gradient(v_loss, self.v_mlp.trainable_variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YVDergPMEIHC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VPGBuffer:\n",
        "  \"\"\"\n",
        "  A buffer for storing trajectories experienced by a VPG agent interacting\n",
        "  with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
        "  for calculating the advantages of state-action pairs.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
        "    self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
        "    self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
        "    self.adv_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.rew_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.ret_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.val_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.logp_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.gamma, self.lam = gamma, lam\n",
        "    self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
        "\n",
        "  def store(self, obs, act, rew, val, logp):\n",
        "    \"\"\"\n",
        "    Append one timestep of agent-environment interaction to the buffer.\n",
        "    \"\"\"\n",
        "    assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
        "    self.obs_buf[self.ptr] = obs\n",
        "    self.act_buf[self.ptr] = act\n",
        "    self.rew_buf[self.ptr] = rew\n",
        "    self.val_buf[self.ptr] = val\n",
        "    self.logp_buf[self.ptr] = logp\n",
        "    self.ptr += 1\n",
        "\n",
        "  def finish_path(self, last_val=0):\n",
        "    \"\"\"\n",
        "    Call this at the end of a trajectory, or when one gets cut off\n",
        "    by an epoch ending. This looks back in the buffer to where the\n",
        "    trajectory started, and uses rewards and value estimates from\n",
        "    the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
        "    as well as compute the rewards-to-go for each state, to use as\n",
        "    the targets for the value function.\n",
        "    The \"last_val\" argument should be 0 if the trajectory ended\n",
        "    because the agent reached a terminal state (died), and otherwise\n",
        "    should be V(s_T), the value function estimated for the last state.\n",
        "    This allows us to bootstrap the reward-to-go calculation to account\n",
        "    for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
        "    \"\"\"\n",
        "\n",
        "    path_slice = slice(self.path_start_idx, self.ptr)\n",
        "    rews = np.append(self.rew_buf[path_slice], last_val)\n",
        "    vals = np.append(self.val_buf[path_slice], last_val)\n",
        "\n",
        "    # the next two lines implement GAE-Lambda advantage calculation\n",
        "    deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
        "    self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
        "\n",
        "    # the next line computes rewards-to-go, to be targets for the value function\n",
        "    self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
        "\n",
        "    self.path_start_idx = self.ptr\n",
        "\n",
        "  def get(self):\n",
        "    \"\"\"\n",
        "    Call this at the end of an epoch to get all of the data from\n",
        "    the buffer, with advantages appropriately normalized (shifted to have\n",
        "    mean zero and std one). Also, resets some pointers in the buffer.\n",
        "    \"\"\"\n",
        "    assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
        "    self.ptr, self.path_start_idx = 0, 0\n",
        "    # the next two lines implement the advantage normalization trick\n",
        "    adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
        "    self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
        "    return self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, self.logp_buf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qdOV-JX-EJ2D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Vanilla Policy Gradient\n",
        "(with GAE-Lambda for advantage estimation)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def vpg(env, ac_kwargs=None, seed=0, steps_per_epoch=4000, epochs=50, gamma=0.99,\n",
        "        clip_ratio=0.2, pi_lr=3e-4, vf_lr=1e-3, train_pi_iters=80, \n",
        "        train_v_iters=80, lam=0.97, max_ep_len=1000, target_kl=0.01, save_freq=10):\n",
        "\n",
        "  tf.random.set_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  # Create actor-critic agent \n",
        "  ac_kwargs['action_space'] = env.action_space\n",
        "  ac_kwargs['pi_lr'] = pi_lr\n",
        "  ac_kwargs['vf_lr'] = vf_lr\n",
        "  ac_kwargs['train_pi_iters'] = train_pi_iters\n",
        "  ac_kwargs['train_v_iters'] = train_v_iters\n",
        "  actor_critic = ActorCritic(**ac_kwargs)\n",
        "\n",
        "  # Experience buffer\n",
        "  obs_dim = env.observation_space.shape\n",
        "  act_dim = env.action_space.shape\n",
        "  buf = VPGBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
        "\n",
        "  \"\"\"\n",
        "  Main loop: collect experience in env and update/log each epoch\n",
        "  \"\"\"\n",
        "\n",
        "  # o for observation, r for reward, d for done\n",
        "  o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "\n",
        "  all_ep_ret = []\n",
        "  summary_ep_ret = []\n",
        "  totalEnvInteracts = []\n",
        "  for epoch in range(epochs):\n",
        "    for t in range(steps_per_epoch):\n",
        "\n",
        "      a, logp_t, v_t = actor_critic(o.reshape(1, -1).astype(np.float32), None)\n",
        "\n",
        "      # save and log\n",
        "      a = a.numpy()[0]\n",
        "      buf.store(o, a, r, v_t, logp_t)\n",
        "\n",
        "      o, r, d, _ = env.step(a)\n",
        "      ep_ret += r\n",
        "      ep_len += 1\n",
        "\n",
        "      terminal = d or (ep_len == max_ep_len)\n",
        "      if terminal or (t == steps_per_epoch - 1):\n",
        "        if not(terminal):\n",
        "          print('Warning: trajectory cut off by epoch at %d steps.' % ep_len)\n",
        "        # if trajectory didn't reach terminal state, bootstrap value target\n",
        "        last_val = r if d else v_t\n",
        "        buf.finish_path(last_val)\n",
        "\n",
        "        if terminal:\n",
        "          all_ep_ret.append(ep_ret)\n",
        "        # reset environment\n",
        "        o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "\n",
        "    # Perform VPG update!\n",
        "    actor_critic.update(buf, clip_ratio, target_kl)\n",
        "    mean, std = statistics_scalar(all_ep_ret)\n",
        "    all_ep_ret = []\n",
        "\n",
        "    print(f'epoch {epoch}: mean {mean}, std {std}')\n",
        "    summary_ep_ret.append(mean)\n",
        "    totalEnvInteracts.append((epoch + 1) * steps_per_epoch)\n",
        "\n",
        "  plt.plot(totalEnvInteracts, summary_ep_ret)\n",
        "  plt.grid(True)\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_QLMDw5jEL3O",
        "colab_type": "code",
        "outputId": "c8611b20-8ad5-4fe8-dd14-bc9746af3ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2056
        }
      },
      "cell_type": "code",
      "source": [
        "vpg(gym.make('CartPole-v0'), ac_kwargs=dict(hidden_sizes=[64] * 2), gamma=0.99, seed=0, steps_per_epoch=4000, epochs=20)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: trajectory cut off by epoch at 13 steps.\n",
            "Early stopping at step 6 due to reaching max kl.\n",
            "epoch 0: mean 16.822784423828125, std 8.292276382446289\n",
            "Warning: trajectory cut off by epoch at 9 steps.\n",
            "Early stopping at step 7 due to reaching max kl.\n",
            "epoch 1: mean 19.75742530822754, std 10.380654335021973\n",
            "Warning: trajectory cut off by epoch at 3 steps.\n",
            "Early stopping at step 11 due to reaching max kl.\n",
            "epoch 2: mean 24.672840118408203, std 12.868677139282227\n",
            "Warning: trajectory cut off by epoch at 32 steps.\n",
            "epoch 3: mean 34.50434875488281, std 20.503816604614258\n",
            "Warning: trajectory cut off by epoch at 51 steps.\n",
            "epoch 4: mean 52.6533317565918, std 39.77385711669922\n",
            "Warning: trajectory cut off by epoch at 61 steps.\n",
            "epoch 5: mean 77.23529052734375, std 55.095848083496094\n",
            "Warning: trajectory cut off by epoch at 43 steps.\n",
            "epoch 6: mean 98.92500305175781, std 52.56728744506836\n",
            "Warning: trajectory cut off by epoch at 144 steps.\n",
            "epoch 7: mean 148.3076934814453, std 53.5750846862793\n",
            "Warning: trajectory cut off by epoch at 15 steps.\n",
            "epoch 8: mean 147.59259033203125, std 50.572471618652344\n",
            "Warning: trajectory cut off by epoch at 5 steps.\n",
            "epoch 9: mean 166.4583282470703, std 44.11063766479492\n",
            "Warning: trajectory cut off by epoch at 54 steps.\n",
            "epoch 10: mean 179.36363220214844, std 40.88514709472656\n",
            "Warning: trajectory cut off by epoch at 125 steps.\n",
            "epoch 11: mean 168.47825622558594, std 30.841629028320312\n",
            "Warning: trajectory cut off by epoch at 58 steps.\n",
            "epoch 12: mean 187.7142791748047, std 34.06037139892578\n",
            "Warning: trajectory cut off by epoch at 98 steps.\n",
            "epoch 13: mean 195.10000610351562, std 15.546381950378418\n",
            "Warning: trajectory cut off by epoch at 62 steps.\n",
            "epoch 14: mean 196.89999389648438, std 13.512587547302246\n",
            "Warning: trajectory cut off by epoch at 99 steps.\n",
            "epoch 15: mean 195.0500030517578, std 14.83399772644043\n",
            "epoch 16: mean 200.0, std 0.0\n",
            "epoch 17: mean 181.81817626953125, std 22.30906105041504\n",
            "Warning: trajectory cut off by epoch at 158 steps.\n",
            "epoch 18: mean 192.10000610351562, std 15.984053611755371\n",
            "Warning: trajectory cut off by epoch at 111 steps.\n",
            "epoch 19: mean 194.4499969482422, std 13.691146850585938\n",
            "Warning: trajectory cut off by epoch at 14 steps.\n",
            "epoch 20: mean 199.3000030517578, std 3.0512290000915527\n",
            "Warning: trajectory cut off by epoch at 34 steps.\n",
            "epoch 21: mean 188.85714721679688, std 35.63209533691406\n",
            "Warning: trajectory cut off by epoch at 22 steps.\n",
            "epoch 22: mean 189.42857360839844, std 35.90937423706055\n",
            "Warning: trajectory cut off by epoch at 30 steps.\n",
            "epoch 23: mean 189.04762268066406, std 24.3045654296875\n",
            "Warning: trajectory cut off by epoch at 47 steps.\n",
            "epoch 24: mean 197.64999389648438, std 10.243412017822266\n",
            "Warning: trajectory cut off by epoch at 57 steps.\n",
            "epoch 25: mean 187.76190185546875, std 19.331457138061523\n",
            "Warning: trajectory cut off by epoch at 139 steps.\n",
            "epoch 26: mean 193.0500030517578, std 14.918025970458984\n",
            "Warning: trajectory cut off by epoch at 74 steps.\n",
            "epoch 27: mean 196.3000030517578, std 12.490396499633789\n",
            "Warning: trajectory cut off by epoch at 199 steps.\n",
            "epoch 28: mean 181.0, std 29.280254364013672\n",
            "Warning: trajectory cut off by epoch at 169 steps.\n",
            "epoch 29: mean 182.42857360839844, std 33.40312576293945\n",
            "Warning: trajectory cut off by epoch at 68 steps.\n",
            "epoch 30: mean 196.60000610351562, std 10.321821212768555\n",
            "Warning: trajectory cut off by epoch at 158 steps.\n",
            "epoch 31: mean 182.95237731933594, std 26.355581283569336\n",
            "Warning: trajectory cut off by epoch at 1 steps.\n",
            "epoch 32: mean 190.42857360839844, std 22.370412826538086\n",
            "Warning: trajectory cut off by epoch at 41 steps.\n",
            "epoch 33: mean 172.13043212890625, std 34.583614349365234\n",
            "Warning: trajectory cut off by epoch at 91 steps.\n",
            "epoch 34: mean 186.14285278320312, std 23.281478881835938\n",
            "Warning: trajectory cut off by epoch at 150 steps.\n",
            "epoch 35: mean 160.4166717529297, std 28.779211044311523\n",
            "Warning: trajectory cut off by epoch at 164 steps.\n",
            "epoch 36: mean 166.78260803222656, std 27.44473648071289\n",
            "Warning: trajectory cut off by epoch at 15 steps.\n",
            "epoch 37: mean 189.76190185546875, std 14.299200057983398\n",
            "Warning: trajectory cut off by epoch at 82 steps.\n",
            "epoch 38: mean 186.57142639160156, std 23.002216339111328\n",
            "Warning: trajectory cut off by epoch at 190 steps.\n",
            "epoch 39: mean 190.5, std 16.779451370239258\n",
            "Warning: trajectory cut off by epoch at 106 steps.\n",
            "epoch 40: mean 177.0, std 29.068883895874023\n",
            "Warning: trajectory cut off by epoch at 8 steps.\n",
            "epoch 41: mean 181.4545440673828, std 29.65977668762207\n",
            "Warning: trajectory cut off by epoch at 112 steps.\n",
            "epoch 42: mean 185.14285278320312, std 24.669790267944336\n",
            "Warning: trajectory cut off by epoch at 38 steps.\n",
            "epoch 43: mean 180.09091186523438, std 24.568843841552734\n",
            "Warning: trajectory cut off by epoch at 169 steps.\n",
            "epoch 44: mean 174.13636779785156, std 29.18171501159668\n",
            "Warning: trajectory cut off by epoch at 49 steps.\n",
            "epoch 45: mean 179.59091186523438, std 24.55357551574707\n",
            "Warning: trajectory cut off by epoch at 39 steps.\n",
            "epoch 46: mean 172.21739196777344, std 33.04010009765625\n",
            "Warning: trajectory cut off by epoch at 155 steps.\n",
            "epoch 47: mean 183.09524536132812, std 24.075090408325195\n",
            "Warning: trajectory cut off by epoch at 88 steps.\n",
            "epoch 48: mean 186.2857208251953, std 22.407785415649414\n",
            "Warning: trajectory cut off by epoch at 22 steps.\n",
            "epoch 49: mean 180.81817626953125, std 23.391210556030273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "B_Ii1aWcMhpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}